{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbaec97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:04:14.718707Z",
     "iopub.status.busy": "2025-04-22T05:04:14.718397Z",
     "iopub.status.idle": "2025-04-22T05:04:56.892787Z",
     "shell.execute_reply": "2025-04-22T05:04:56.891581Z"
    },
    "papermill": {
     "duration": 42.183043,
     "end_time": "2025-04-22T05:04:56.894630",
     "exception": false,
     "start_time": "2025-04-22T05:04:14.711587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "jupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, which is not installed.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n",
    "!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc91a210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:04:56.908013Z",
     "iopub.status.busy": "2025-04-22T05:04:56.907660Z",
     "iopub.status.idle": "2025-04-22T05:05:01.098547Z",
     "shell.execute_reply": "2025-04-22T05:05:01.097643Z"
    },
    "papermill": {
     "duration": 4.199518,
     "end_time": "2025-04-22T05:05:01.100197",
     "exception": false,
     "start_time": "2025-04-22T05:04:56.900679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from IPython.display import Markdown\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939ee886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:01.113731Z",
     "iopub.status.busy": "2025-04-22T05:05:01.112585Z",
     "iopub.status.idle": "2025-04-22T05:05:01.461168Z",
     "shell.execute_reply": "2025-04-22T05:05:01.460156Z"
    },
    "papermill": {
     "duration": 0.3568,
     "end_time": "2025-04-22T05:05:01.462809",
     "exception": false,
     "start_time": "2025-04-22T05:05:01.106009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fb4882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:01.476515Z",
     "iopub.status.busy": "2025-04-22T05:05:01.476088Z",
     "iopub.status.idle": "2025-04-22T05:05:02.147613Z",
     "shell.execute_reply": "2025-04-22T05:05:02.146483Z"
    },
    "papermill": {
     "duration": 0.680161,
     "end_time": "2025-04-22T05:05:02.149305",
     "exception": false,
     "start_time": "2025-04-22T05:05:01.469144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "for m in client.models.list():\n",
    "    if \"embedContent\" in m.supported_actions:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b28880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:02.162101Z",
     "iopub.status.busy": "2025-04-22T05:05:02.161718Z",
     "iopub.status.idle": "2025-04-22T05:05:02.169573Z",
     "shell.execute_reply": "2025-04-22T05:05:02.168254Z"
    },
    "papermill": {
     "duration": 0.016119,
     "end_time": "2025-04-22T05:05:02.171240",
     "exception": false,
     "start_time": "2025-04-22T05:05:02.155121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create embedding function for ChromaDB\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    document_mode = True\n",
    "\n",
    "    @retry.Retry(predicate=lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503}))\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            contents=input,\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=embedding_task,\n",
    "            ),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eceda6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:02.185004Z",
     "iopub.status.busy": "2025-04-22T05:05:02.184626Z",
     "iopub.status.idle": "2025-04-22T05:05:02.238800Z",
     "shell.execute_reply": "2025-04-22T05:05:02.237689Z"
    },
    "papermill": {
     "duration": 0.063132,
     "end_time": "2025-04-22T05:05:02.240523",
     "exception": false,
     "start_time": "2025-04-22T05:05:02.177391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample FAQ data:\n",
      "                                              source  \\\n",
      "0  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "1  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "2  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "3  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "4  https://www.ipcc.ch/site/assets/uploads/2020/0...   \n",
      "\n",
      "                                                 faq text_type  \n",
      "0  If Understanding of the Climate System Has Inc...         q  \n",
      "1  The models used to calculate the IPCC’s temper...         a  \n",
      "2               How Do We Know the World Has Warmed?         q  \n",
      "3  Evidence for a warming world comes from multip...         a  \n",
      "4   Have There Been Any Changes in Climate Extremes?         q  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/climate-change-faqs/climate_change_faqs.csv')\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample FAQ data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2fbfa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:02.254325Z",
     "iopub.status.busy": "2025-04-22T05:05:02.253644Z",
     "iopub.status.idle": "2025-04-22T05:05:02.279525Z",
     "shell.execute_reply": "2025-04-22T05:05:02.278505Z"
    },
    "papermill": {
     "duration": 0.034492,
     "end_time": "2025-04-22T05:05:02.281293",
     "exception": false,
     "start_time": "2025-04-22T05:05:02.246801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Process FAQ data\n",
    "# Create a dictionary to map questions to answers using the source as a key\n",
    "faq_dict = {}\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "# First pass to collect all questions and answers by source\n",
    "for _, row in df.iterrows():\n",
    "    source = row['source']\n",
    "    content = row['faq']\n",
    "    content_type = row['text_type']\n",
    "    \n",
    "    if source not in faq_dict:\n",
    "        faq_dict[source] = {'q': [], 'a': []}\n",
    "    \n",
    "    faq_dict[source][content_type].append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc36a8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:02.294583Z",
     "iopub.status.busy": "2025-04-22T05:05:02.294233Z",
     "iopub.status.idle": "2025-04-22T05:05:02.302296Z",
     "shell.execute_reply": "2025-04-22T05:05:02.301362Z"
    },
    "papermill": {
     "duration": 0.016296,
     "end_time": "2025-04-22T05:05:02.303675",
     "exception": false,
     "start_time": "2025-04-22T05:05:02.287379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 188 FAQ pairs\n"
     ]
    }
   ],
   "source": [
    "# Second pass to pair questions with answers and prepare documents\n",
    "doc_id = 0\n",
    "for source, content_dict in faq_dict.items():\n",
    "    questions = content_dict.get('q', [])\n",
    "    answers = content_dict.get('a', [])\n",
    "    \n",
    "    # Match questions with answers\n",
    "    for i in range(min(len(questions), len(answers))):\n",
    "        question = questions[i]\n",
    "        answer = answers[i]\n",
    "        \n",
    "        # Format the document\n",
    "        document = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "        documents.append(document)\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            \"source\": source,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "        metadatas.append(metadata)\n",
    "        \n",
    "        # Create unique ID\n",
    "        ids.append(str(doc_id))\n",
    "        doc_id += 1\n",
    "\n",
    "print(f\"Processed {len(documents)} FAQ pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02fa2d",
   "metadata": {
    "papermill": {
     "duration": 0.005451,
     "end_time": "2025-04-22T05:05:02.315000",
     "exception": false,
     "start_time": "2025-04-22T05:05:02.309549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f092c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:02.327831Z",
     "iopub.status.busy": "2025-04-22T05:05:02.327512Z",
     "iopub.status.idle": "2025-04-22T05:05:09.250440Z",
     "shell.execute_reply": "2025-04-22T05:05:09.249415Z"
    },
    "papermill": {
     "duration": 6.931249,
     "end_time": "2025-04-22T05:05:09.252019",
     "exception": false,
     "start_time": "2025-04-22T05:05:02.320770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents 0 to 49...\n",
      "Adding documents 50 to 99...\n",
      "Adding documents 100 to 149...\n",
      "Adding documents 150 to 187...\n",
      "Total documents in database: 188\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True  # Set to document mode for indexing\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection_name = \"climate_faq_db\"\n",
    "\n",
    "# Delete collection if it exists (for clean restart)\n",
    "try:\n",
    "    chroma_client.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection\n",
    "db = chroma_client.create_collection(name=collection_name, embedding_function=embed_fn)\n",
    "\n",
    "# Add documents to the database in smaller batches to avoid potential errors\n",
    "BATCH_SIZE = 50\n",
    "for i in range(0, len(documents), BATCH_SIZE):\n",
    "    end_idx = min(i + BATCH_SIZE, len(documents))\n",
    "    print(f\"Adding documents {i} to {end_idx-1}...\")\n",
    "    \n",
    "    batch_docs = documents[i:end_idx]\n",
    "    batch_metadata = metadatas[i:end_idx]\n",
    "    batch_ids = ids[i:end_idx]\n",
    "    \n",
    "    db.add(\n",
    "        documents=batch_docs,\n",
    "        metadatas=batch_metadata,\n",
    "        ids=batch_ids\n",
    "    )\n",
    "\n",
    "print(f\"Total documents in database: {db.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3c4ebd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:09.265684Z",
     "iopub.status.busy": "2025-04-22T05:05:09.265334Z",
     "iopub.status.idle": "2025-04-22T05:05:09.272495Z",
     "shell.execute_reply": "2025-04-22T05:05:09.271629Z"
    },
    "papermill": {
     "duration": 0.015882,
     "end_time": "2025-04-22T05:05:09.274121",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.258239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to query the FAQ database\n",
    "def query_faq_database(query, n_results=3):\n",
    "    # Switch to query mode for searching\n",
    "    embed_fn.document_mode = False\n",
    "    \n",
    "    # Search the database\n",
    "    results = db.query(\n",
    "        query_texts=[query], \n",
    "        n_results=min(n_results, db.count())\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to determine confidence level\n",
    "def calculate_confidence(distance):\n",
    "    # Convert distance to confidence score (0-1)\n",
    "    # Lower distance means higher confidence\n",
    "    if distance is None:\n",
    "        return 0.5  # Default confidence if distance is not available\n",
    "    return max(0, min(1, 1 - distance / 2))\n",
    "\n",
    "# Define thresholds for agent handoff\n",
    "CONFIDENCE_THRESHOLD = 0.75  # Minimum confidence required\n",
    "UNCERTAIN_THRESHOLD = 0.65   # Below this is uncertain, trigger potential handoff\n",
    "\n",
    "# Function to generate response with Gemini model\n",
    "def generate_response(prompt, model=\"gemini-2.0-flash\"):\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=prompt\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return f\"I'm sorry, I encountered an error processing your request. Please try again.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62ec634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:09.288427Z",
     "iopub.status.busy": "2025-04-22T05:05:09.287717Z",
     "iopub.status.idle": "2025-04-22T05:05:09.301519Z",
     "shell.execute_reply": "2025-04-22T05:05:09.300324Z"
    },
    "papermill": {
     "duration": 0.023232,
     "end_time": "2025-04-22T05:05:09.303601",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.280369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to handle customer query with structured JSON output\n",
    "def handle_customer_query(query, include_json=True):\n",
    "    # Get relevant FAQ matches\n",
    "    results = query_faq_database(query)\n",
    "    \n",
    "    # Check if we have results\n",
    "    if not results[\"documents\"] or len(results[\"documents\"][0]) == 0:\n",
    "        # No relevant documents found\n",
    "        return {\n",
    "            \"answer\": \"I don't have specific information to answer your question accurately. Let me connect you with a human agent who can help you better.\",\n",
    "            \"confidence_score\": 0.0,\n",
    "            \"needs_human_handoff\": True,\n",
    "            \"reference_sources\": [],\n",
    "            \"uncertain_response\": True\n",
    "        }\n",
    "    \n",
    "    documents = results[\"documents\"][0]\n",
    "    distances = results.get(\"distances\", [[0.5] * len(documents)])[0]  # Default distance if not available\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "    \n",
    "    # Calculate confidence scores\n",
    "    confidence_scores = [calculate_confidence(dist) for dist in distances]\n",
    "    max_confidence = max(confidence_scores) if confidence_scores else 0\n",
    "    \n",
    "    # Determine if we need human handoff\n",
    "    needs_human_handoff = max_confidence < CONFIDENCE_THRESHOLD\n",
    "    uncertain_response = max_confidence < UNCERTAIN_THRESHOLD\n",
    "    \n",
    "    # Build context for the model\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        context += f\"Reference {i+1} (Confidence: {confidence_scores[i]:.2f}):\\n{doc}\\n\\n\"\n",
    "    \n",
    "    # Prepare prompt based on confidence level\n",
    "    if max_confidence >= CONFIDENCE_THRESHOLD:\n",
    "        prompt_template = f\"\"\"You are a helpful climate science assistant. Answer the question using the provided references.\n",
    "If the references don't contain enough information, say you don't have enough information.\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "REFERENCES:\n",
    "{context}\n",
    "\n",
    "Generate your response in a friendly, conversational tone. Include relevant facts from the references.\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt_template = f\"\"\"You are a helpful climate science assistant that works with human agents.\n",
    "Based on the query and references below, you need to:\n",
    "1. Try to provide a helpful preliminary response based on available information\n",
    "2. Acknowledge that you're not entirely confident in your answer\n",
    "3. Mention that you're connecting the customer to a human agent for better assistance\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "REFERENCES:\n",
    "{context}\n",
    "\n",
    "Generate your response in a friendly, conversational tone.\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response using Gemini model\n",
    "    if include_json:\n",
    "        # Add structured output format instruction\n",
    "        prompt_template += \"\"\"\n",
    "Return your response in a JSON format with the following structure:\n",
    "{\n",
    "  \"answer\": \"Your answer to the question\",\n",
    "  \"confidence_score\": float between 0 and 1,\n",
    "  \"needs_human_handoff\": boolean,\n",
    "  \"reference_sources\": [\"list of source URLs used\"],\n",
    "  \"uncertain_response\": boolean\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "    response_text = generate_response(prompt_template)\n",
    "    \n",
    "    # If JSON output was requested, parse it\n",
    "    if include_json:\n",
    "        try:\n",
    "            # Try to extract and parse JSON from the response\n",
    "            json_str = response_text.strip()\n",
    "            if '```json' in json_str:\n",
    "                json_str = json_str.split('```json')[1].split('```')[0].strip()\n",
    "            elif '```' in json_str:\n",
    "                json_str = json_str.split('```')[1].split('```')[0].strip()\n",
    "                \n",
    "            result = json.loads(json_str)\n",
    "            \n",
    "            # Add sources if not present\n",
    "            if \"reference_sources\" not in result or not result[\"reference_sources\"]:\n",
    "                result[\"reference_sources\"] = [meta.get(\"source\", \"\") for meta in metadatas]\n",
    "                \n",
    "            # Ensure all fields are present\n",
    "            result.setdefault(\"confidence_score\", max_confidence)\n",
    "            result.setdefault(\"needs_human_handoff\", needs_human_handoff)\n",
    "            result.setdefault(\"uncertain_response\", uncertain_response)\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSON response: {e}\")\n",
    "            # Fallback to text response with added fields\n",
    "            return {\n",
    "                \"answer\": response_text,\n",
    "                \"confidence_score\": max_confidence,\n",
    "                \"needs_human_handoff\": needs_human_handoff,\n",
    "                \"reference_sources\": [meta.get(\"source\", \"\") for meta in metadatas],\n",
    "                \"uncertain_response\": uncertain_response\n",
    "            }\n",
    "    else:\n",
    "        # Return regular text response\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c1b86",
   "metadata": {
    "papermill": {
     "duration": 0.005911,
     "end_time": "2025-04-22T05:05:09.316370",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.310459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## agent handoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcfcb2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:09.330277Z",
     "iopub.status.busy": "2025-04-22T05:05:09.329873Z",
     "iopub.status.idle": "2025-04-22T05:05:09.336636Z",
     "shell.execute_reply": "2025-04-22T05:05:09.335732Z"
    },
    "papermill": {
     "duration": 0.015992,
     "end_time": "2025-04-22T05:05:09.338275",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.322283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Live agent handoff function\n",
    "def transfer_to_human_agent(query, customer_info, ai_response):\n",
    "    \"\"\"\n",
    "    Simulate transferring the conversation to a human agent\n",
    "    In a real implementation, this would integrate with your support ticket system\n",
    "    \"\"\"\n",
    "    ticket = {\n",
    "        \"query\": query,\n",
    "        \"customer_info\": customer_info,\n",
    "        \"ai_response\": ai_response,\n",
    "        \"ticket_id\": f\"TICKET-{random.randint(10000, 99999)}\",\n",
    "        \"status\": \"open\",\n",
    "        \"priority\": \"medium\",\n",
    "        \"assigned_to\": \"next_available_agent\",\n",
    "        \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # In a real system, you would:\n",
    "    # 1. Create a ticket in your support system\n",
    "    # 2. Queue for next available agent\n",
    "    # 3. Transfer the chat context\n",
    "    \n",
    "    return ticket\n",
    "\n",
    "# Sample FAQ for few-shot examples\n",
    "sample_faqs = [\n",
    "    {\n",
    "        \"question\": \"How Do We Know the World Has Warmed?\",\n",
    "        \"answer\": \"Evidence for a warming world comes from multiple independent climate indicators, from high up in the atmosphere to the depths of the oceans. They include changes in surface, atmospheric and oceanic temperatures, glaciers, snow cover, sea ice, sea level and atmospheric water vapor.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Have There Been Any Changes in Climate Extremes?\",\n",
    "        \"answer\": \"Since about 1950, changes in many extreme weather and climate events have been observed. Some of these changes have been linked to human influences, including a decrease in cold temperature extremes, an increase in warm temperature extremes, an increase in extreme high sea levels and an increase in the number of heavy precipitation events in various regions.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640def86",
   "metadata": {
    "papermill": {
     "duration": 0.006074,
     "end_time": "2025-04-22T05:05:09.350667",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.344593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d1a82b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:09.363925Z",
     "iopub.status.busy": "2025-04-22T05:05:09.363579Z",
     "iopub.status.idle": "2025-04-22T05:05:09.371243Z",
     "shell.execute_reply": "2025-04-22T05:05:09.370234Z"
    },
    "papermill": {
     "duration": 0.016053,
     "end_time": "2025-04-22T05:05:09.372715",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.356662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_few_shot_prompt(query):\n",
    "    few_shot_template = \"\"\"I'll demonstrate how to answer climate science questions with accurate information:\n",
    "\n",
    "Example 1:\n",
    "Question: {question1}\n",
    "Answer: {answer1}\n",
    "\n",
    "Example 2:\n",
    "Question: {question2}\n",
    "Answer: {answer2}\n",
    "\n",
    "Now, please answer this question in a similar style:\n",
    "Question: {user_query}\n",
    "\"\"\".format(\n",
    "        question1=sample_faqs[0][\"question\"],\n",
    "        answer1=sample_faqs[0][\"answer\"],\n",
    "        question2=sample_faqs[1][\"question\"],\n",
    "        answer2=sample_faqs[1][\"answer\"],\n",
    "        user_query=query\n",
    "    )\n",
    "    \n",
    "    return few_shot_template\n",
    "\n",
    "# Enhanced function for handling complex queries with few-shot prompting\n",
    "def handle_complex_query(query):\n",
    "    # First try standard RAG approach\n",
    "    standard_result = handle_customer_query(query)\n",
    "    \n",
    "    # If confidence is low, try few-shot approach\n",
    "    if standard_result[\"confidence_score\"] < 0.6:\n",
    "        few_shot_prompt = get_few_shot_prompt(query)\n",
    "        \n",
    "        # Get relevant FAQ matches\n",
    "        results = query_faq_database(query)\n",
    "        if results[\"documents\"] and len(results[\"documents\"][0]) > 0:\n",
    "            documents = results[\"documents\"][0]\n",
    "            context = \"\\n\\n\".join(documents)\n",
    "            \n",
    "            # Create enhanced prompt with few-shot examples and context\n",
    "            enhanced_prompt = f\"{few_shot_prompt}\\n\\nRelevant information:\\n{context}\"\n",
    "            \n",
    "            # Generate improved response\n",
    "            response_text = generate_response(enhanced_prompt)\n",
    "            \n",
    "            # Update result\n",
    "            standard_result[\"answer\"] = response_text\n",
    "            standard_result[\"confidence_score\"] += 0.1  # Slightly boost confidence\n",
    "    \n",
    "    return standard_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99627305",
   "metadata": {
    "papermill": {
     "duration": 0.005786,
     "end_time": "2025-04-22T05:05:09.384933",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.379147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7ccdcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:09.400326Z",
     "iopub.status.busy": "2025-04-22T05:05:09.399918Z",
     "iopub.status.idle": "2025-04-22T05:05:09.415835Z",
     "shell.execute_reply": "2025-04-22T05:05:09.414724Z"
    },
    "papermill": {
     "duration": 0.024944,
     "end_time": "2025-04-22T05:05:09.417426",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.392482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total FAQs available: 376\n",
      "Questions: 188\n",
      "Answers: 188\n",
      "\n",
      "Type run_customer_support_demo() to start the interactive demo\n",
      "Type demo_structured_output() to see JSON structured output example\n"
     ]
    }
   ],
   "source": [
    "# Demo function to test the system\n",
    "def run_customer_support_demo():\n",
    "    print(\"🌍 Climate Science Customer Support AI Demo\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Type 'exit' to quit the demo\")\n",
    "    \n",
    "    # Keep conversation history\n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\n👤 Customer: \")\n",
    "        if query.lower() in ['exit', 'quit']:\n",
    "            print(\"Thank you for using our demo!\")\n",
    "            break\n",
    "            \n",
    "        # Add to conversation history\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Mock customer info (in production this would come from your system)\n",
    "        customer_info = {\n",
    "            \"customer_id\": \"demo-user-123\",\n",
    "            \"name\": \"Demo User\",\n",
    "            \"contact\": \"demo@example.com\"\n",
    "        }\n",
    "        \n",
    "        # Process the query\n",
    "        print(\"\\n⏳ Processing query...\")\n",
    "        \n",
    "        # Try complex query handling for better responses\n",
    "        result = handle_complex_query(query)\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"\\n🤖 AI Assistant: {result['answer']}\")\n",
    "        \n",
    "        # Add to conversation history\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": result['answer']})\n",
    "        \n",
    "        # Show metadata (for demo purposes)\n",
    "        print(f\"\\n📊 Response metadata:\")\n",
    "        print(f\"Confidence: {result['confidence_score']:.2f}\")\n",
    "        print(f\"Sources: {', '.join(result['reference_sources'])}\")\n",
    "        \n",
    "        # Handle agent transfer if needed\n",
    "        if result['needs_human_handoff']:\n",
    "            print(\"\\n🔄 Transferring to human agent...\")\n",
    "            ticket = transfer_to_human_agent(query, customer_info, result)\n",
    "            print(f\"\\n👨‍💼 Human Agent: Hello! I'm taking over from our AI assistant (Ticket #{ticket['ticket_id']})\")\n",
    "            print(\"How can I help you further with your question about climate science?\")\n",
    "            \n",
    "            # Exit the loop after human handoff\n",
    "            print(\"\\nDemo ended after human handoff. Type 'run_customer_support_demo()' to start a new session.\")\n",
    "            break\n",
    "\n",
    "# Demo function for testing structured JSON output\n",
    "def demo_structured_output():\n",
    "    query = \"What does the IPCC say about sea level rise?\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Get response with structured output\n",
    "    result = handle_customer_query(query, include_json=True)\n",
    "    \n",
    "    # Print formatted JSON\n",
    "    print(\"Structured JSON Output:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    # Show how this can be used programmatically\n",
    "    if result[\"needs_human_handoff\"]:\n",
    "        print(\"\\nSystem would automatically transfer to human agent\")\n",
    "    else:\n",
    "        print(\"\\nAI can handle this query confidently\")\n",
    "\n",
    "# Run the demo\n",
    "if __name__ == \"__main__\":\n",
    "    # Print info about the data\n",
    "    print(f\"\\nTotal FAQs available: {len(df)}\")\n",
    "    print(f\"Questions: {len(df[df['text_type'] == 'q'])}\")\n",
    "    print(f\"Answers: {len(df[df['text_type'] == 'a'])}\")\n",
    "    print(\"\\nType run_customer_support_demo() to start the interactive demo\")\n",
    "    print(\"Type demo_structured_output() to see JSON structured output example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54884bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T05:05:09.431431Z",
     "iopub.status.busy": "2025-04-22T05:05:09.431107Z",
     "iopub.status.idle": "2025-04-22T05:05:11.307031Z",
     "shell.execute_reply": "2025-04-22T05:05:11.305926Z"
    },
    "papermill": {
     "duration": 1.884878,
     "end_time": "2025-04-22T05:05:11.308714",
     "exception": false,
     "start_time": "2025-04-22T05:05:09.423836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What does the IPCC say about sea level rise?\n",
      "\n",
      "Structured JSON Output:\n",
      "{\n",
      "  \"answer\": \"Based on the IPCC reports, sea levels are expected to continue rising due to melting ice and thermal expansion of the ocean as it warms. The rate of sea level rise has been increasing. Depending on future emissions, sea level rise could be between 29-59 cm or up to 1 meter by 2100 relative to 1986-2005 levels. There's also a risk of much larger increases from the melting of ice sheets in Greenland and West Antarctica, which could eventually lead to several meters of sea level rise.\",\n",
      "  \"confidence_score\": 0.7,\n",
      "  \"needs_human_handoff\": true,\n",
      "  \"reference_sources\": [\n",
      "    \"https://www.imperial.ac.uk/grantham/publications/climate-change-faqs/\",\n",
      "    \"https://www.theguardian.com/environment/series/the-ultimate-climate-change-faq\",\n",
      "    \"https://www.theguardian.com/environment/series/the-ultimate-climate-change-faq\"\n",
      "  ],\n",
      "  \"uncertain_response\": true\n",
      "}\n",
      "\n",
      "System would automatically transfer to human agent\n"
     ]
    }
   ],
   "source": [
    "demo_structured_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f3fa5",
   "metadata": {
    "papermill": {
     "duration": 0.006586,
     "end_time": "2025-04-22T05:05:11.321721",
     "exception": false,
     "start_time": "2025-04-22T05:05:11.315135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Things to improve\n",
    "Proper implementation of live agent hang off"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    },
    {
     "datasetId": 1714346,
     "sourceId": 2805752,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.732785,
   "end_time": "2025-04-22T05:05:12.550265",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-22T05:04:09.817480",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
